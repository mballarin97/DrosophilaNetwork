\section{Coarse graining}
It is necessary to define rigorously a way to apply the 
coarse graining procedure, i.e. our way to combine different
neurons in super-neurons. We stress that this procedure
is not simple at all, and present different degrees of freedom
that can (and will) be chosen with respect to the performances
on the data. We start by defining these degrees of freedom:
\begin{itemize}
    \item the \tb{metric} $d_{ij}$ that we use to define when two 
        different neurons are similar;
    \item The way in which we aggregate the weights after we combine 
        two nodes.
\end{itemize}
We will so present two different algorithms that approaches in different ways the second 
degree of freedom. We stress that the first algorithm is not really working, but it is 
however instructing analyzing why it is so.

\subsection{Random aggregation}
We so present the steps of the first algorithms. Calling $n_i^{(\alpha)}$ the $i$-th 
neuron at the $\alpha$-th iteration in the coarse graining algorithm and $w_{ij}^{(\alpha)}$
the links between neurons $n_i^{(\alpha)}$, $n_j^{(\alpha)}$:
\begin{enumerate}
    \item Pick a neuron $n_i^{(\alpha)}$ at random;
    \item Compute the distance $d_i^{(\alpha)}=(d_{i1}^{(\alpha)}, d_{i2}^{(\alpha)}, \dots, d_{iN}^{(\alpha)})$;
    \item Combine the two nearest neurons:
        $$
        n_i^{(\alpha+1)} = \left\{ n_i^{(\alpha)}, \min_{d_i}\left[n_k^{(\alpha)}\right] \right\}
        $$
    \item Build the new network connections:
        $$
        w_{ij}^{(\alpha+1)} = \left\{ w_{ij}^{(\alpha)}, \bar{w}_{kj}^{(\alpha)}   \right\}
        $$
        where $\bar{w}_{kj}^{(\alpha)}$ are a random subset of the connections $w^{(\alpha)}_{kj}$ such that 
        the density of the network is conserved.
    \item Start again from point 1.
\end{enumerate} 
This constraint on the density was needed, since it avoids a proliferation of connections.
It is, however, a bound that is not easy to fulfill. In particular, it can be implemented 
by randomly extract the correct number of connections. This does not work in all cases, since
not all nodes have the same degree: there are a lot of nodes we very small degree, that have 
less connections than the ones needed. Another problem of this algorithm is that it is an
iterative algorithm and it is so really slow: at each iteration we need to compute all the 
distances and we can eliminate only a neuron at each iteration.

\subsection{Clustering aggregation \label{subsec:clust}}
The second algorithm is simpler and more effective. We make use of a hierarchical agglomerative
clustering technique, i.e. an algorithm that recursively merges the pair of clusters that 
minimally increases a given linkage distance. This is really similar to the idea developed 
for the community detection using the dendogram technique.
We so perform the following steps:
\begin{itemize}
    \item Select the  final number of neurons $N'$, i.e. the number of clusters for the agglomerative
        clustering;
    \item Compute the distance matrix $d_{ij}$;
    \item Apply the clustering algorithm;
    \item Combine nodes in the same cluster in a supernode $I$, which is connected to another 
        supernode $J$ if at least a node in $I$ was connected in a node in $J$. The weight of the 
        connection is the sum of the weights fo the single nodes, i.e.:
        $$
        W_{IJ} = \sum_{k\in I, l\in J} w_{kl}
        $$
\end{itemize}
This algorithm has several advantages over the previous one:
\begin{itemize}
    \item We do not have to impose the density constraint to avoid the proliferation of the 
        connections;
    \item We have to compute the distance matrix only once. This can not seem an advantage.
        In the random algorithm we had to compute the distance from a node $N\cdot (N-N')$,
        where $N$ is the total number of nodes and $N'$ the final number of nodes after the 
        coarse graining, while in the clustering case we must compute the distance $N^2$ times.
        However, if we want to try different $N'$ or run the algorithm multiple times in the 
        first case we should start from scratch for each graining, while in the second case
        we only need to compute the distance matrix once;
    \item The clustering algorithm used is optimized for even larger systems, and it is so really
        fast.
\end{itemize}


We decided to use two different metrics for this procedure. We only have to remember that 
we are minimizing a distance in these algorithms, so strongly connected nodes will be 
nodes considered nearer. 
\begin{enumerate}
    \item The weights, with 
        $$d^{w}_{ij}=\frac{1}{w_{ij}}$$
        This is a really naive distance metric, and we do not expect a really good performance
        for the algorithm. However, it has the advantage of being already computed;
    \item The local page rank, with:
        $$d^{pr}_{ij}=\frac{1}{\mbox{pr}_{ij}}$$
        This is a measure often used in network science, and takes into account both the outcoming
        and incoming links. We expect much better results with this metric.
\end{enumerate}

We present in this report only the results for the clustering algorithm using the local page rank
metric. This choice has been done to keep the report readable, focusing only on the interesting
results.

\subsection{Metrics evolution}
We will mainly focus on five different aspects to analyze along the graining procedure, shown in
Figure \ref{fig:coarse_evol}:
\begin{itemize}
    \item The exponent of the degree power-law distribution $\alpha$, where $p(k)=\beta k^{-\alpha}$. We notice that, starting from a coefficient
        of $3.6$, which is the usual one for this type of networks, we increase this coefficient up to 
        $5.5$, where it remains almost constant. This means that the degree distribution become steeper 
        and steeper, showing a really quick change between low-degree nodes and hubs. We are so losing 
        the nodes “in the middle”. This is an interesting result, since it confirms what we were discussing
        in Subsection \ref{subsec:clust}: the hubs are not destroyed by the algorithm.
    \item The density of the network, which is defined as:
        \begin{equation}
            \rho^{(t)}=\frac{\# \mbox{ of edges}}{(\# \mbox{ of nodes})^2}
        \end{equation}
        The density of the network increases really slowly initially, but grows quickly when we 
        reach lower number of neurons, after $5000$. This behavior is sensible, since we are really
        zipping the graph structure at these stages, and when the number of neurons goes to $0$ also
        the number of edges does so, i.e.:
        $$
        \lim_{N'\rightarrow 0} \rho = 1
        $$
    \item The average degree. In this case the actual average is not the more informative measure. It is really more 
        significant its standard deviation. We can indeed see that the variance of the degree is decreasing 
        along our graining, as we have already understood for the power-law distribution. 
    \item The average degree connectivity, which gives a measure of how much 
        the neighbors of a node with a given degree are connected. We see that, after an initial 
        increase it linearly decreases, with some fluctuation in the variance. This graph is the hardest to
        interpret, and we will so analyze more in detail the average degree connectivity distribution in the 
        following.
\end{itemize}
However, in all the measures analyzed we can point out an outlier, with $3000$ neurons. 
We so plot in Figure \ref{fig:n3000} the degree distribution and the degree connectivity distribution in this case.
We can see an anomalous peak in the degree distribution, which fools the automatic computation of the 
power law degree coefficient, but we can clearly observe that the distribution follows the distribution
with coefficient $\alpha=-5$. The degree connectivity distribution follows instead an interesting bimodal
distribution.

We then show the evolution of the degree connectivity distribution, in Figure \ref{fig:con_distr}.
We see that the first hint of the bimodal distribution at $10500$ neurons, and it becomes then more 
visible, as we have also seen in Figure \ref{fig:n3000}. This is again due to the coarse graining, which
removes the nodes that are in the middle from the degree point of view, dividing the network in hubs
and lowly connected nodes.

We have finally understood, from many metrics, the effect of the coarse graining on the network structure:
the elimination of the nodes in the middle.