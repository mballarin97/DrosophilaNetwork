\section{Coarse graining \label{sec:coarse}}
As we have stated in the introduction, the size of the network is overwhelming.
We will so apply a procedure to combine different neurons into super-neurons, 
called coarse graining. We stress that this procedure
is not simple at all, and present different degrees of freedom
that can (and will) be chosen with respect to the performances
on the data. We start by defining these degrees of freedom:
\begin{itemize}
    \item the \tb{metric} $d_{ij}$ that we use to how two 
        different neurons are far away from each other;
    \item The way in which we aggregate the synaptic weights after we combine 
        two nodes.
\end{itemize}
We will so present two different algorithms that approaches in different ways the second 
degree of freedom. We underline that the first algorithm is not performing well, but it is 
however instructing analyzing why it is so.

\subsection{Random aggregation}
Calling $n_i^{(\alpha)}$ the $i$-th 
neuron at the $\alpha$-th iteration in the coarse graining algorithm and $w_{ij}^{(\alpha)}$
the links between neurons $n_i^{(\alpha)}$, $n_j^{(\alpha)}$ we perform the following
steps:
\begin{enumerate}
    \item Select randomly a neuron $n_i^{(\alpha)}$;
    \item Compute the distance $d_i^{(\alpha)}=(d_{i1}^{(\alpha)}, d_{i2}^{(\alpha)}, \dots, d_{iN}^{(\alpha)})$
        of that neuron from all the others;
    \item Combine the two nearest neurons:
        $$
        n_i^{(\alpha+1)} = \left\{ n_i^{(\alpha)}, \min_{d_i}\left[n_k^{(\alpha)}\right] \right\}
        $$
    \item Build the new network connections:
        $$
        w_{ij}^{(\alpha+1)} = \left\{ w_{ij}^{(\alpha)}, \bar{w}_{kj}^{(\alpha)}   \right\}
        $$
        where $\bar{w}_{kj}^{(\alpha)}$ are a random subset of the connections $w^{(\alpha)}_{kj}$ such that 
        the density of the network is conserved.
    \item Start again from point 1.
\end{enumerate} 
The constraint on the network density was needed to avoid a proliferation of connections.
It is, however, a bound that is not easy to fulfill. It can be implemented 
by randomly extract the correct number of connections, but it does not work in all cases, since
not all nodes have the same degree: there are a lot of nodes with very small degree, that have 
fewer connections than the ones needed. Another problem of this algorithm is that it is an
iterative algorithm and it is so really slow: at each iteration we need to compute all the 
distances, and we can eliminate only a neuron at each iteration.

\subsection{Clustering aggregation \label{subsec:clust}}
The second algorithm is simpler and more effective. We make use of a hierarchical agglomerative
clustering technique \cite{scikit-learn}, i.e. an algorithm that recursively merges the pair of clusters that 
minimally increases a given linkage distance. This is really similar to the idea developed 
for the community detection using the dendogram technique.
We so perform the following steps:
\begin{enumerate}
    \item Select the  final number of neurons $N'$, i.e. the number of clusters for the agglomerative
        clustering;
    \item Compute the distance matrix $d_{ij}$ between each neuron;
    \item Apply the clustering algorithm;
    \item Combine nodes in the same cluster in a supernode $I$, which is connected to another 
        supernode $J$ if at least a node in $I$ was connected in a node in $J$. The weight of the 
        connection is the sum of the weights fo the single nodes, i.e.:
        $$
        W_{IJ} = \sum_{k\in I, l\in J} w_{kl}
        $$
\end{enumerate}
This algorithm has several advantages over the previous one:
\begin{itemize}
    \item We do not have to impose the density constraint to avoid the proliferation of the 
        connections;
    \item We have to compute the distance matrix only once. This can not seem an advantage.
        In the random algorithm we had to compute the distance from a node $N\cdot (N-N')$,
        where $N$ is the total number of nodes and $N'$ the final number of nodes after the 
        coarse graining, while in the clustering case we must compute the distance $N^2$ times.
        However, if we want to try different $N'$ or run the algorithm multiple times in the 
        first case we should start from scratch for each graining, while in the second case
        we only need to compute the distance matrix once;
    \item The clustering algorithm used is optimized for even larger systems, and it is so really
        fast.
    \item Also in this case we can give a connectivity constraint through the connectivity
     matrix.
\end{itemize}


We decided to use two different metrics for this procedure. We only have to remember that 
we are minimizing a distance in these algorithms, so strongly connected nodes will be 
nodes considered nearer. The metrics are the followings:
\begin{enumerate}
    \item The weights, with distance:
        $$d^{w}_{ij}=\frac{1}{w_{ij}}$$
        This is a really naive distance metric, and we do not expect a really good performance
        for the algorithm. However, it has the advantage of being already computed;
    \item The local page rank pr$_{ij}$, with distance:
        $$d^{pr}_{ij}=\frac{1}{\mbox{pr}_{ij}}$$
        This is a measure often used in network science, and takes into account both the outcoming
        and incoming links. We expect much better results with this metric.
\end{enumerate}

We present in this report only the results for the clustering algorithm using the local page rank
metric. This choice has been done to keep the report shorter, focusing only on the interesting
results.

\subsection{Metrics evolution}
We will mainly focus on five different aspects to analyze along the graining procedure, shown in
Figure \ref{fig:coarse_evol}:
\begin{itemize}
    \item The \tb{exponent of the degree power-law distribution} $\alpha$, where the 
        distribution is $p(k)=\beta k^{-\alpha}$. We notice that, starting from a coefficient
        of $3.6$, which is the usual one for this type of networks, we increase this coefficient up to 
        $5.5$, where it remains almost constant. This means that the degree distribution become steeper 
        and steeper, showing a really quick change between low-degree nodes and hubs. We are so losing 
        the nodes “in the middle”. This is an interesting result, since it confirms that 
        the hubs are not destroyed by the algorithm.
    \item The \tb{density of the network}, which is defined as:
        \begin{equation}
            \rho^{(t)}=\frac{\# \mbox{ of edges}}{(\# \mbox{ of nodes})^2}
        \end{equation}
        It increases really slowly initially, but grows quickly when we 
        reach lower number of neurons, after $5000$. This behavior is sensible, since we are really
        zipping the graph structure at these stages, and when the number of neurons goes to $0$ also
        the number of edges does so, i.e.:
        $$
        \lim_{N'\rightarrow 0} \rho = 1
        $$
    \item The \tb{average degree}. In this case the actual average is not the more informative measure. It is really more 
        significant its standard deviation. We can indeed see that the variance of the degree is decreasing 
        along our graining, as we have already understood from the power-law coefficient, since
        we are eliminating all the “middle-degree” nodes. 
    \item The \tb{average degree connectivity}, which gives a measure of how much 
        the neighbors of a node with a given degree are connected. We see that, after an initial 
        increase it linearly decreases, with some fluctuation in the variance. This graph is the hardest to
        interpret, and we will so analyze more in detail the average degree connectivity distribution in the 
        following.
\end{itemize}
However, in all the measures analyzed we can point out an outlier, with $3000$ neurons. 
We so plot in Figure \ref{fig:n3000} the degree distribution and the degree connectivity distribution of this outlier.
We can see an anomalous peak in the degree distribution, which fools the automatic computation of the 
power law degree coefficient, but we can clearly observe that the distribution follows a power law
with coefficient $\alpha=-5$. The degree connectivity distribution is, instead, interestingly bimodal.

We so focus on the evolution of the degree connectivity distribution, shown in Figure \ref{fig:con_distr}.
We see that the bi-modality arises at $10500$ neurons, and it becomes then more 
visible, as we have also seen in Figure \ref{fig:n3000}. This is again due to the coarse graining, which
removes the nodes that are in the middle from the degree point of view, dividing the network in hubs
and lowly connected nodes.

We have finally understood, from many metrics, the effect of the coarse graining on the network structure:
the elimination of the nodes with “middle-degree”.