---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# Network analysis
import networkx as nx
from scipy import sparse
from fast_pagerank import pagerank_power
# Data handling
import pandas as pd
# Visualization
import matplotlib.pyplot as plt
#import igraph as ig
#import cairocffi as cairo
# Miscellaneous
from tqdm import tqdm
import time
import numpy as np
```

```{python}
# False -> avoid running time-consuming cells
PATIENCE = False
```

## Importing datasets
- neurons_dataframe contains neuron number (body_id). The body_id is an [unique identifier](https://en.wikipedia.org/wiki/Unique_identifier#:~:text=A%20unique%20identifier%20(UID)%20is,with%20an%20atomic%20data%20type.). It also contains the neuron cell type (type) and the neuron unique name (instance).
- synapses_dataframe contains the sparse connectivity matrix of the network;
- roi_dataframe contains also the region of interest of the connection, but some of them are repeated, i.e. we have the same couple of connecting neurons in different regions (maybe they are not sure)

https://asajadi.github.io/fast-pagerank/

```{python}
neurons_dataframe = pd.read_csv('exported-traced-adjacencies/traced-neurons.csv')
synapses_dataframe = pd.read_csv('exported-traced-adjacencies/traced-total-connections.csv')
roi_dataframe = pd.read_csv('exported-traced-adjacencies/traced-roi-connections.csv')
```

```{python}
len( neurons_dataframe['type'].unique() ) 
neurons_dataframe.head()
```

```{python}
# add link as 2-tuple between neurons bodyId
synapses_dataframe['synaps'] = list(zip(synapses_dataframe.bodyId_pre, synapses_dataframe.bodyId_post))
print(synapses_dataframe.head())
print(synapses_dataframe['synaps'].nunique() )
print(len(synapses_dataframe['synaps']))
```

```{python}
# Visualization of 
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(synapses_dataframe['weight'], bins=bins)
x = (x[1:]+x[:-1])/2
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log weight log(w)')
plt.ylabel('Log Frequency ')
plt.title('Weight distribution')
plt.grid()
plt.show()
```

#### Hemibrain regions of interest

```{python}
roi_dataframe.head()
```

```{python}
print(roi_dataframe['roi'].unique() )
print(len(roi_dataframe))
```

```{python}
(4218945 - 3521163)/3521163
```

## Graph

```{python}
graph = nx.Graph()
graph.add_nodes_from(neurons_dataframe['bodyId'])
graph.add_edges_from(synapses_dataframe['synaps'])
```

#### Adjacency matrix

```{python}
matfig = plt.figure(figsize=(20,20))
plt.matshow(nx.adjacency_matrix(graph).todense(), fignum=matfig.number, cmap=plt.get_cmap("binary"))
```

#### Non-randomness coefficient

```{python}
if PATIENCE:
    start_time = time.time()
    loc, glob = nx.non_randomness(graph)
    print(glob)
    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Degree distribution and properties


See https://stackoverflow.com/questions/49908014/how-can-i-check-if-a-network-is-scale-free and https://pypi.org/project/powerlaw/

I'm not completely sure about the parameters.

```{python}
degree = np.array([ d for n, d in graph.degree()])
print(degree.mean())
```

```{python}
# used for degree distribution and powerlaw test
# reverse = True to have the cumulative distribution
degree_sequence = sorted([d for n, d in graph.degree()], reverse=True) 
# Power laws are probability distributions with the form:p(x)âˆxâˆ’Î±
import powerlaw 
# NOTE: xmin is the data value beyond which distributions should be fitted. 
# If None an optimal one will be calculated
fit = powerlaw.Fit(degree_sequence) 
fig2 = fit.plot_pdf(color='b', linewidth=2)
fit.power_law.plot_pdf(color='g', linestyle='--', ax=fig2)
plt.legend(["data", "power law"])
R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)
print (f"Loglikelihood: {R:.2f}, p-value: {p:.2e}")
```

```{python}
plt.figure(figsize=(10, 6))
fit.distribution_compare('power_law', 'lognormal')
fig4 = fit.plot_ccdf(linewidth=3, color='black')
fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--') #powerlaw
fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--') #lognormal
fit.stretched_exponential.plot_ccdf(ax=fig4, color='b', linestyle='--') #stretched_exponential
plt.legend(["data", "power law", "log-normal", "stretched_exponential"])
```

```{python}
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(degree, bins=bins)
x = (x[1:]+x[:-1])/2
```

```{python}
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

# Coarse graining 
To apply our coarse graining procedure we aggregate the nodes with their nearest neighbors, where the concept of nearest is choosen using one of the following centrality measures:
- **local page rank**. The local page rank gives the proximity of the nodes with respect to the chosen node. So the second highest node is gonna be merged with the one where the lpr is computed. Networkx creates each time the sparse matrix, which is a time consuming task. We need to compute the page rank several times, and so chosen to use another representation;
- **link strength**. We merge the more connected or the less connected (with the idea that they are not influencing the graph properties)

```{python}
def delete_from_csr(mat, row_indices=[], col_indices=[]):
    """
    Remove the rows (denoted by ``row_indices``) and columns (denoted by ``col_indices``) from the CSR sparse matrix ``mat``.
    WARNING: Indices of altered axes are reset in the returned matrix
    """
    if not isinstance(mat, csr_matrix):
        raise ValueError("works only for CSR format -- use .tocsr() first")

    rows = []
    cols = []
    if row_indices:
        rows = list(row_indices)
    if col_indices:
        cols = list(col_indices)

    if len(rows) > 0 and len(cols) > 0:
        row_mask = np.ones(mat.shape[0], dtype=bool)
        row_mask[rows] = False
        col_mask = np.ones(mat.shape[1], dtype=bool)
        col_mask[cols] = False
        return mat[row_mask][:,col_mask]
    elif len(rows) > 0:
        mask = np.ones(mat.shape[0], dtype=bool)
        mask[rows] = False
        return mat[mask]
    elif len(cols) > 0:
        mask = np.ones(mat.shape[1], dtype=bool)
        mask[cols] = False
        return mat[:,mask]
    else:
        return mat

class CoarseGrainerSM:
    def __init__(self, nodes, connectivity, threshold=0):
        """
            Parameters:
                nodes: np array size (N,) with node name where N is number of nodes
                connectivity: sparse csr matrix
        """
        self.nodes = nodes
        self.connectivity = connectivity
        self.initial_N = self.nodes.size
        self.initial_M = self.connectivity.shape[0]
        self.threshold = threshold
        print(f'Initialized with connectivity matrix of density: {self.initial_M/self.initial_N**2}')
        
    def set_metrics(self, metric_vect, node_id, minmax):
        """ 
            Define the metric vector and the node_id to be used at the next iteration
            Parameters:
                node_id (string): Name of the selected node
                metric_vect (np array(1, B)): metric of the network wrt @node_id
                minmax(int): 1 if looking for max of the metric, -1 if looking for min
        """
        self.sel_node_id = node_id
        self.metric = metric_vect
        self.minmax = minmax
        
    def fusion(self):
        nearest_node = np.argmax( self.minmax*self.metric )
        nearest_node_id = self.nodes[nearest_node]
        # Eliminate the nearest node from the node list
        self.nodes = np.delete(self.nodes, nearest_node)
        
        # Join connection of the deleted node to the other
        self.connectivity.getrow(self.sel_node) += self.connectivity.getrow(nearest_node)
        # Avoid self loops
        self.connectivity.getrow(self.sel_node)[self.sel_node] = 0
        # Delete old node
        self.connectivity = delete_from_csr(self.connectivity, nearest_node, nearest_node)
    
    def _get_weight(self, node_id):
        temp1 = self.connectivity[:,2][ self.connectivity[:,0]==node_id ].flatten()
        temp2 = self.connectivity[:,2][ self.connectivity[:,1]==node_id ].flatten()
        return np.hstack( (temp1, temp2))
    
    def _test(self, Niter):
        for i in tqdm(range(Niter)):
            node_id = np.random.choice(self.nodes)
            metric_vect = self._get_weight(node_id)
            self.set_metrics(metric_vect, node_id, +1)
            self.fusion()
        
```

```{python}
class CoarseGrainer:
    def __init__(self, nodes, connectivity, threshold=0):
        """
            Parameters:
                nodes: np array size (N,) with node name where N is number of nodes
                connectivity: np array size (M, 3) where M is the number of links. We have M<<N**2
        """
        self.nodes = nodes
        self.connectivity = connectivity
        self.N = self.nodes.size
        self.M = self.connectivity.shape[0]
        self.threshold = threshold
        print(f'Initialized with connectivity matrix of density: {self.M/self.N**2}')
        
    def set_metrics(self, metric_vect, node_id, minmax):
        """ 
            Define the metric vector and the node_id to be used at the next iteration
            Parameters:
                node_id (string): Name of the selected node
                metric_vect (np array(1, B)): metric of the network wrt @node_id
                minmax(int): 1 if looking for max of the metric, -1 if looking for min
        """
        self.sel_node = node_id
        self.metric = metric_vect
        self.minmax = minmax
        
    def fusion(self):
        nearest_node = np.argmax( self.minmax*self.metric )
        nearest_node_id = self.nodes[nearest_node]
        # Eliminate the nearest node from the node list
        self.nodes = np.delete(self.nodes, nearest_node)
        
        dM = int( self.M*(2/self.N-1/self.N**2) )
        
        connections = np.hstack( (np.where(self.connectivity[:,0]==nearest_node_id),
                                  np.where(self.connectivity[:,1]==nearest_node_id) ) )
        connections = np.unique(connections)
        connections_selected = np.random.choice(connections, size=max(1, connections.size-dM), replace=False)
        connestions_not_selected = connections[connections != connections_selected]
        # Join connection of the deleted node to the other
        self.connectivity[:,0][ connections_selected] = self.sel_node
        self.connectivity[:,1][ connections_selected] = self.sel_node
        
        # Eliminate self loops and non-selected connections
        self.connectivity = self.connectivity[ self.connectivity[:,0] != self.connectivity[:,1] ]
        self.connectivity = np.delete(self.connectivity, np.minimum(connestions_not_selected, self.M), axis=0)
        
        self.N -= 1 
        self.M -= dM
        
    def _get_weight(self, node_id):
        temp1 = self.connectivity[:,2][ self.connectivity[:,0]==node_id ].flatten()
        temp2 = self.connectivity[:,2][ self.connectivity[:,1]==node_id ].flatten()
        return np.hstack( (temp1, temp2))
    
    def _test(self, Niter):
        for i in tqdm(range(Niter)):
            node_id = np.random.choice(self.nodes)
            metric_vect = self._get_weight(node_id)
            self.set_metrics(metric_vect, node_id, +1)
            self.fusion()
        print(f'Finalized with connectivity matrix of density: {self.M/self.N**2}')
```

```{python}
nodes = np.array(neurons_dataframe['bodyId'])
M = len(synapses_dataframe['bodyId_pre'])
connectivity = np.hstack(  (np.array(synapses_dataframe['bodyId_pre']).reshape(M,1), 
                            np.array(synapses_dataframe['bodyId_post']).reshape(M,1), 
                           np.array( synapses_dataframe['weight']).reshape(M,1))
    )
```

```{python}
coarse = CoarseGrainer(nodes, connectivity)
```

```{python}
start_time = time.time()
coarse._test(10000)
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
coarse.connectivity.shape[0]/coarse.nodes.size**2
```

```{python}
synapses= list(zip(coarse.connectivity[:,0], coarse.connectivity[:,1]))
graph_coarsed = nx.Graph()
graph_coarsed.add_nodes_from(coarse.nodes)
graph_coarsed.add_edges_from(synapses)
```

```{python}
degree = np.array([ d for n, d in graph_coarsed.degree()])
print(np.mean(degree))
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(degree, bins=bins)
x = (x[1:]+x[:-1])/2

# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

```{python}
matfig = plt.figure(figsize=(20,20))
adj = nx.adjacency_matrix(graph_coarsed).todense()
plt.matshow(adj, fignum=matfig.number, cmap=plt.get_cmap("binary"))
```

#### PageRank measure

```{python}
SM = nx.convert_matrix.to_scipy_sparse_matrix(graph)
```

```{python}
start_time = time.time()
locality = np.zeros(len(neurons_dataframe))
locality[0] = 1        
page=pagerank_power(SM, p=0.85, personalize=locality, tol=1e-6)
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
        
    plt.hist(page.values(), bins=100)
    plt.show()
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank_scipy(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
    
    plt.hist(page.values(), bins=100)
    plt.show()
```

## Approximate visualization of main brain regions

```{python}
names = ['GNG', 'PENP', 'VMNP', 'CX', 'LX', 'AL', 'MB', 'INP', 'VLNP', 'OL', 'SNP', 'LH']
links = [('GNG', 'PENP'), ('PENP', 'VMNP'), ('PENP', 'CX'), ('PENP', 'LX'), ('VMNP', 'CX'),
         ('VMNP', 'LX'), ('CX', 'LX'), ('CX', 'AL'), ('LX', 'AL'), ('AL', 'INP'), 
         ('AL', 'MB'), ('INP', 'VLNP'), ('VLNP', 'OL'), ('MB', 'SNP'), ('SNP', 'LH')]
```

```{python}
high_lvl_brain = nx.Graph()
high_lvl_brain.add_nodes_from(names)
high_lvl_brain.add_edges_from(links)
high_lvl_brain_ig = ig.Graph.from_networkx(high_lvl_brain)
```

```{python}
colors = ig.drawing.colors.known_colors
colors = list(colors.keys())
print(colors)
```

```{python}
#set label to be names of nx graph nodes
high_lvl_brain_ig.vs["label"] = high_lvl_brain_ig.vs["_nx_name"]

visual_style = {}
#node size
visual_style["vertex_size"] = 20
#node color
c = [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0]
n_names = np.array(['deep sky blue', 'magenta4'])
visual_style["vertex_color"] = n_names[c]  
#node label
visual_style["vertex_label"] = high_lvl_brain_ig.vs["label"]
#node label color
visual_style["vertex_label_color"] = "black"
#node label size
visual_style["vertex_label_size"] = 15
#edge thickness
visual_style["edge_width"] = 2
#bounding box
visual_style["bbox"] = (500, 500)
#margin
visual_style["margin"] = 20

ig.plot(high_lvl_brain_ig, "high_lvl_brain.pdf", **visual_style, layout="kk")
```

# To do list
1. âœ”ï¸ list brain regions to create different $12$ theoretical communities;
1. âœ”ï¸ understand what the $12$ regions does; 
1. ðŸŸ¥ Understand brain section of neurons;
1. âœ”ï¸ Check degree distribution is power law
1. â—»ï¸ netorkx documentation (algorithms in particular);
1. â—»ï¸ Local connectivity measure;
1. â—»ï¸ Colour adjacency matrix based on connection supergroup **FRANK**;
1. â—»ï¸ Redefine ROI by taking connection with highest weight only **FRANK**;
1. â—»ï¸ Define coarse graining procedure **BALLA**;
1. â—»ï¸ Curare il report up to now;
1. â—»ï¸ Better define coarse graining metrics;
1. â—»ï¸ Community detection (naive and after coarse graining);
1. â—»ï¸ Betweenness (especially for multiple ROI synapses), centrality, small world characteristics;
1. âœ”ï¸Distance from random network;
1. â—»ï¸ robustness to cuts. In particular it may be interesting to attack outer regions of the brain, the more easily damaged. The  **common connectome constraint paper** already say something about it;


###### Problems
In point (3), we have that the same synapse (same link between two neurons, as we've defined it here) may belong to more than one region. We could add only the one with the highest weight, but often is not possible. Random?


## Average measures


#### Degree

```{python}
degree = np.array([ d for n, d in graph.degree()])
degree_mean = int(degree.mean())
print(f"Degree mean: {degree_mean}")
y, x = np.histogram(degree, bins=100)
x = (x[1:]+x[:-1])/2
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

#### Degree connectivity
The *average degree connectivity* is the average nearest neighbor degree of nodes with degree k. 

```{python}
avg_degree_connectivity = nx.average_degree_connectivity(graph)
avg_degree_connectivity = np.array([i for k,i in avg_degree_connectivity.items()])
print(f"Global average degree connectivity value: {int(avg_degree_connectivity.mean())}")

plt.hist(avg_degree_connectivity, bins=200)
plt.grid()
plt.title("Average degree connectivity distribution")
plt.xlabel("Average degree connectivity for degree k")
plt.show()
```

#### Average clustering coefficient
Time saver: avg_clustering_coeff = 0.31370364316752

```{python}
if PATIENCE:
    start_time = time.time()

    avg_clustering_coeff = nx.average_clustering(graph)
    print(f"Average clustering coefficient: {avg_clustering_coeff}")

    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Average shortest path length (NOTE: veeeery long)

```{python}
if PATIENCE:
    # sanity check
    if nx.is_connected(graph):
        start_time = time.time()

        avg_shortest_path = nx.average_shortest_path_length(graph)
        print(avg_shortest_path)

        end_time = time.time()
        print("Total time: {:.2f} seconds".format((end_time - start_time)))
    else:
        print("Graph is not connected.")
```

```{python}

```
