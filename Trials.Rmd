---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# Network analysis
import networkx as nx
from scipy import sparse
from fast_pagerank import pagerank_power
# Data handling
import pandas as pd
# Visualization
import matplotlib.pyplot as plt
#import igraph as ig
#import cairocffi as cairo
# Miscellaneous
from tqdm import tqdm
import time
import numpy as np
```

```{python}
# False -> avoid running time-consuming cells
PATIENCE = False
```

## Importing datasets
- neurons_dataframe contains neuron number (body_id). The body_id is an [unique identifier](https://en.wikipedia.org/wiki/Unique_identifier#:~:text=A%20unique%20identifier%20(UID)%20is,with%20an%20atomic%20data%20type.). It also contains the neuron cell type (type) and the neuron unique name (instance).
- synapses_dataframe contains the sparse connectivity matrix of the network;
- roi_dataframe contains also the region of interest of the connection, but some of them are repeated, i.e. we have the same couple of connecting neurons in different regions (maybe they are not sure)

https://asajadi.github.io/fast-pagerank/

```{python}
neurons_dataframe = pd.read_csv('exported-traced-adjacencies/traced-neurons.csv')
synapses_dataframe = pd.read_csv('exported-traced-adjacencies/traced-total-connections.csv')
roi_dataframe = pd.read_csv('exported-traced-adjacencies/traced-roi-connections.csv')
```

```{python}
len( neurons_dataframe['type'].unique() ) 
neurons_dataframe.head()
```

```{python}
# add link as 2-tuple between neurons bodyId
synapses_dataframe['synaps'] = list(zip(synapses_dataframe.bodyId_pre, synapses_dataframe.bodyId_post))
print(synapses_dataframe.head())
print(synapses_dataframe['synaps'].nunique() )
print(len(synapses_dataframe['synaps']))
```

```{python}
# Visualization of 
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(synapses_dataframe['weight'], bins=bins)
x = (x[1:]+x[:-1])/2
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log weight log(w)')
plt.ylabel('Log Frequency ')
plt.title('Weight distribution')
plt.grid()
plt.show()
```

#### Hemibrain regions of interest

```{python}
roi_dataframe.head()
```

```{python}
print(roi_dataframe['roi'].unique() )
print(len(roi_dataframe))
```

```{python}
(4218945 - 3521163)/3521163
```

## Graph

```{python}
graph = nx.Graph()
graph.add_nodes_from(neurons_dataframe['bodyId'])
graph.add_edges_from(synapses_dataframe['synaps'])
```

#### Adjacency matrix

```{python}
matfig = plt.figure(figsize=(20,20))
plt.matshow(nx.adjacency_matrix(graph).todense(), fignum=matfig.number, cmap=plt.get_cmap("binary"))
```

#### Non-randomness coefficient

```{python}
if PATIENCE:
    start_time = time.time()
    loc, glob = nx.non_randomness(graph)
    print(glob)
    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Degree distribution and properties


See https://stackoverflow.com/questions/49908014/how-can-i-check-if-a-network-is-scale-free and https://pypi.org/project/powerlaw/

I'm not completely sure about the parameters.

```{python}
degree = np.array([ d for n, d in graph.degree()])
print(degree.mean())
```

```{python}
# used for degree distribution and powerlaw test
# reverse = True to have the cumulative distribution
degree_sequence = sorted([d for n, d in graph.degree()], reverse=True) 
# Power laws are probability distributions with the form:p(x)‚àùx‚àíŒ±
import powerlaw 
# NOTE: xmin is the data value beyond which distributions should be fitted. 
# If None an optimal one will be calculated
fit = powerlaw.Fit(degree_sequence) 
fig2 = fit.plot_pdf(color='b', linewidth=2)
fit.power_law.plot_pdf(color='g', linestyle='--', ax=fig2)
plt.legend(["data", "power law"])
R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)
print (f"Loglikelihood: {R:.2f}, p-value: {p:.2e}")
```

```{python}
plt.figure(figsize=(10, 6))
fit.distribution_compare('power_law', 'lognormal')
fig4 = fit.plot_ccdf(linewidth=3, color='black')
fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--') #powerlaw
fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--') #lognormal
fit.stretched_exponential.plot_ccdf(ax=fig4, color='b', linestyle='--') #stretched_exponential
plt.legend(["data", "power law", "log-normal", "stretched_exponential"])
```

```{python}
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(degree, bins=bins)
x = (x[1:]+x[:-1])/2
```

```{python}
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

# Coarse graining 
To apply our coarse graining procedure we aggregate the nodes with their nearest neighbors, where the concept of nearest is choosen using one of the following centrality measures:
- **local page rank**. The local page rank gives the proximity of the nodes with respect to the chosen node. So the second highest node is gonna be merged with the one where the lpr is computed. Networkx creates each time the sparse matrix, which is a time consuming task. We need to compute the page rank several times, and so chosen to use another representation;
- **link strength**. We merge the more connected or the less connected (with the idea that they are not influencing the graph properties)

```{python}
class CoarseGrainer:
    def __init__(self, nodes, connectivity):
        """
            Parameters:
                nodes: np array size (N,) with node name where N is number of nodes
                connectivity: np array size (M, 3) where M is the number of links. We have M<<N**2
        """
        self.nodes = nodes
        self.connectivity = connectivity
        self.N = self.nodes.size
        self.M = self.connectivity.shape[0]
        self.powerlaw = np.array([])
        self.avg_deg_conn = np.array([])
        self.density = [self.M/self.N**2]
        print(f'Initialized with connectivity matrix of density: {self.M/self.N**2}')
        
    def set_metrics(self, metric_vect, node_id, minmax):
        """ 
            Define the metric vector and the node_id to be used at the next iteration
            Parameters:
                node_id (string): Name of the selected node
                metric_vect (np array(2, B)): (connected_node ,metric) of the network wrt @node_id
                minmax(int): 1 if looking for max of the metric, -1 if looking for min
        """
        self.sel_node = node_id
        self.metric = metric_vect
        self.minmax = minmax
        
    def fusion(self, min_links=10):
        """
            Select the nearest node to @self.sel_node. Fuse the two node togheter by keeping the density
            of connections constant, selecting randomly the links to keep among the possible ones.
            Eliminate self loops
        """
        if self.metric.size != 0:
            # Select nearest node
            nearest_node_id = self.metric[:, 0][ np.argmax( self.minmax*self.metric[:, 1] ) ]
            nearest_node = np.where( self.nodes==nearest_node_id )
            # Eliminate the nearest node from the node list
            self.nodes = np.delete(self.nodes, nearest_node)
            # Compute difference in connectivity to keep the density fixed
            dM = int( self.M*(2/self.N-1/self.N**2) )
            # Select the connections of the nearest node
            connections = np.hstack( (np.where(self.connectivity[:,0]==nearest_node_id),
                                      np.where(self.connectivity[:,1]==nearest_node_id) ) )
            connections = np.unique(connections) # keep only the uniques ones
            # Randomly select the connections by keeping the density fixed
            min_links = min( min_links, connections.size)
            connections_selected = np.random.choice(connections, size=max(min_links, connections.size-dM), replace=False)
            # (----?----) Add selection with probability proportional to the weight (----?----)
            # Select the non-selected connections
            connestions_not_selected = np.setdiff1d(connections, connections_selected)
            # Join connection of the deleted node to the other
            self.connectivity[:,0][ connections_selected] = self.sel_node
            self.connectivity[:,1][ connections_selected] = self.sel_node

            # Eliminate non-selected connections and self loops
            self.connectivity = np.delete(self.connectivity, connestions_not_selected, axis=0)
            self.connectivity = self.connectivity[ self.connectivity[:,0] != self.connectivity[:,1] ]
            
            # Update nodes number and connectivity
            self.N -= 1 
            self.M = self.connectivity.shape[0]
        
    def _get_weight(self, node_id):
        """
            Get the weights of the links connecting @node_id to other nodes. In particular returns
            an array of size (2, B) where the first column is the ID of the connected node and
            the second column contains the weights
        """
        temp1 = self.connectivity[:, 1:3][ self.connectivity[:,0]==node_id ]
        temp2 = self.connectivity[:, 0:3:2][ self.connectivity[:,1]==node_id ]
        return np.vstack( (temp1, temp2))
    
    def _upd_stat(self):
        """
            Updates the statistics. Notice that this function is very time consuming since we need
            to instantiate the graph object. In partiucular we compute:
                - The coefficient of the power law of the degree distribution
                - The average of the degree connectivity
        """
        # Graph instantiaton
        synapses= list(zip(self.connectivity[:,0], self.connectivity[:,1]))
        graph_coarsed = nx.Graph()
        graph_coarsed.add_nodes_from(self.nodes)
        graph_coarsed.add_edges_from(synapses)
        # Degree distribution
        degree = sorted([d for n, d in graph_coarsed.degree()], reverse=True)
        fit = powerlaw.Fit(degree) 
        if self.powerlaw.size==0:
            self.powerlaw = np.array([fit.alpha, fit.sigma])
        else:
            self.powerlaw = np.vstack( (self.powerlaw, [fit.alpha, fit.sigma]) )
        # Degree connectivity
        avg_degree_connectivity = nx.average_degree_connectivity(graph_coarsed)
        avg_degree_connectivity = np.array([i for k,i in avg_degree_connectivity.items()])
        self.avg_deg_conn = np.append(self.avg_deg_conn, int(avg_degree_connectivity.mean()) )
        
    def plot_stat(self):
        """
            Plot the two statistics collected in the experiment.
        """
        fig, ax = plt.subplots(1, 2, figsize=(12,8))
        ax[0].set_xlabel('Iteration $10^3$')
        ax[0].set_ylabel('Coefficient of the power law')
        ax[0].set_title('Evolution of the power law distribution')
        ax[0].plot(self.powerlaw[:,0], 'o--', color='blue', label='Coef')
        ax[0].fill_between(np.arange(self.powerlaw.shape[0]), (self.powerlaw[:,0]+self.powerlaw[:,1]),
                           (self.powerlaw[:,0]-self.powerlaw[:,1]), alpha=0.3, label='Error', color='green' )
        ax[0].legend()
        
        ax[1].set_xlabel('Iteration $10^3$')
        ax[1].set_ylabel('Average degree connectivity')
        ax[1].set_title('Evolution of the average degree connectivity')
        ax[1].plot(self.avg_deg_conn, 'o--', color='blue')
        plt.show()
        
        
    def _test(self, Niter, Nupd_stat=1000, min_links=10):
        """
            Apply the coarse graining for @Niter iterations and updating the statistics each @Nupd_stat iter
        """
        for i in tqdm(range(Niter)):
            node_id = np.random.choice(self.nodes)
            metric_vect = self._get_weight(node_id)
            self.set_metrics(metric_vect, node_id, +1)
            self.fusion(min_links=min_links)
            if i%Nupd_stat == 0:
                self._upd_stat()
            self.density.append(self.M/self.N**2)
        print(f'Finalized with connectivity matrix of density: {self.M/self.N**2}')
```

```{python}
nodes = np.array(neurons_dataframe['bodyId'])
M = len(synapses_dataframe['bodyId_pre'])
connectivity = np.hstack(  (np.array(synapses_dataframe['bodyId_pre']).reshape(M,1), 
                            np.array(synapses_dataframe['bodyId_post']).reshape(M,1), 
                           np.array( synapses_dataframe['weight']).reshape(M,1))
    )
```

```{python}
coarse = CoarseGrainer(nodes, connectivity)
```

```{python}
start_time = time.time()
coarse._test(20000)
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
coarse.plot_stat()
```

```{python}
d = np.array( coarse.density[0:-1:200] )
plt.plot(d)
plt.show()
```

```{python}
coarse.M/coarse.N**2
```

```{python}
synapses= list(zip(coarse.connectivity[:,0], coarse.connectivity[:,1]))
graph_coarsed = nx.Graph()
graph_coarsed.add_nodes_from(coarse.nodes)
graph_coarsed.add_edges_from(synapses)
```

```{python}
degree = np.array([ d for n, d in graph_coarsed.degree()])
print(np.mean(degree))
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(degree, bins=bins)
x = (x[1:]+x[:-1])/2

# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

```{python}
matfig = plt.figure(figsize=(20,20))
adj = nx.adjacency_matrix(graph_coarsed).todense()
plt.matshow(adj, fignum=matfig.number, cmap=plt.get_cmap("binary"))
plt.show()
```

## Coarse graining through clustering

```{python}
from community import induced_graph
from sklearn.cluster import AgglomerativeClustering
```

```{python}
class ClusterGrainer():
    def __init__(self, graph, n_clusters):
        self.G = graph
        self.adj = nx.adjacency_matrix(self.G).todense()
        self.nodes = np.array([n for n in self.G.nodes()])
        self.clusterer = AgglomerativeClustering(n_clusters=n_clusters, affinity= 'precomputed', 
                                                 connectivity =np.sign(self.adj), linkage='single')
        # Statistics
        degree = sorted([d for n, d in self.G.degree()], reverse=True)
        avg_degree_con = nx.average_degree_connectivity(self.G)
        avg_degree_con = np.array([i for k,i in avg_degree_con.items()])
        fit = powerlaw.Fit(degree) 
        self.avg_deg = np.array( [np.mean(degree), np.std(degree)]  )
        self.avg_deg_con =  np.array( [int(avg_degree_con.mean()), avg_degree_con.std()] )
        self.power_law = np.array( [fit.alpha, fit.sigma] )
        self.density = np.array(self.G.number_of_edges()/self.G.number_of_nodes()**2 )
        
    def predict(self, metric_mat):
        clusts = self.clusterer.fit_predict(metric_mat)
        part = { n : c for n, c in zip(self.nodes, clusts) }
        return part
    
    def zip_graph(self, partitions, inplace=True):
        if inplace:
            self.G = induced_graph(partitions, self.G)
            self.adj = nx.adjacency_matrix(self.G).todense()
            self.nodes = np.array([n for n in self.G.nodes()])
        else:
            return induced_graph(partitions, self.G)
        
    def _upd_stats(self, graph ):
        degree = sorted([d for n, d in graph.degree()], reverse=True)
        avg_degree_con = nx.average_degree_connectivity(graph)
        avg_degree_con = np.array([i for k,i in avg_degree_con.items()])
        fit = powerlaw.Fit(degree) 
        
        self.avg_deg = np.vstack( (self.avg_deg, [np.mean(degree), np.std(degree)])  )
        self.avg_deg_con = np.vstack( (self.avg_deg_con, [int(avg_degree_con.mean()), avg_degree_con.std()] ))
        self.power_law = np.vstack( (self.power_law, [fit.alpha, fit.sigma]) )
        self.density = np.append( self.density, graph.number_of_edges()/graph.number_of_nodes()**2 )
        
    def _set_clust(self, n_cl):
        self.clusterer = AgglomerativeClustering(n_clusters=n_cl, affinity= 'precomputed', 
                                                 connectivity =np.sign(self.adj), linkage='single')
        
    def experiment(self, n_clusters, metric_mat, inplace=False):
        for cl in tqdm(n_clusters):
            self._set_clust(cl)
            partitions = self.predict(metric_mat)
            new_graph= self.zip_graph(partitions, inplace=inplace)
            self._upd_stats(new_graph)
    
    def plot_stats(self, sizes):
        fig, ax = plt.subplots(2, 2, figsize=(12, 12))
        ax = ax.flatten()
        ax[0].set_xlabel('Size of the network [# of nodes]')
        ax[0].set_ylabel('Coefficient of the power law')
        ax[0].set_title('Evolution of the power law distribution')
        ax[0].plot(sizes, self.power_law[:,0], 'o--', color='blue', label='Coef')
        ax[0].fill_between(sizes, (self.power_law[:,0]+self.power_law[:,1]),
                           (self.power_law[:,0]-self.power_law[:,1]), alpha=0.3, label='Error', color='green' )
        ax[0].legend()
        
        ax[1].set_xlabel('Size of the network [# of nodes]')
        ax[1].set_ylabel('Density #edges/(#nodes)^2')
        ax[1].set_title('Evolution of the network density')
        ax[1].plot(sizes, self.density, 'o--', color='blue')
        
        ax[2].set_xlabel('Size of the network [# of nodes]')
        ax[2].set_ylabel('Average degree')
        ax[2].set_title('Evolution of the average degree')
        ax[2].plot(sizes, self.avg_deg[:,0], 'o--', color='blue', label='Average')
        ax[2].fill_between(sizes, (self.avg_deg[:,0]+self.avg_deg[:,1]),
                           (self.avg_deg[:,0]-self.avg_deg[:,1]), alpha=0.3, label='Error', color='green' )
        ax[2].legend()
        
        ax[3].set_xlabel('Size of the network [# of nodes]')
        ax[3].set_ylabel('Average degree connectivity')
        ax[3].set_title('Evolution of the average degree connectivity')
        ax[3].plot(sizes, self.avg_deg_con[:,0], 'o--', color='blue', label='Average')
        ax[3].fill_between(sizes, (self.avg_deg_con[:,0]+self.avg_deg_con[:,1]),
                           (self.avg_deg_con[:,0]-self.avg_deg_con[:,1]), alpha=0.3, label='Error', color='green' )
        ax[3].legend()
        
        plt.show()
```

```{python}
graph = nx.Graph()
graph.add_nodes_from(neurons_dataframe['bodyId'])
graph.add_edges_from(synapses_dataframe['synaps'])
Grainer = ClusterGrainer(graph, 10000)
```

```{python}
clusters = np.arange(19000, 0, -1000)
weight_metric = Grainer.adj
```

```{python}
Grainer.experiment(clusters, weight_metric)
```

```{python}
sizes = np.insert(clusters, 0, graph.number_of_nodes())
Grainer.plot_stats(sizes)
```

#### PageRank measure

```{python}
SM = nx.convert_matrix.to_scipy_sparse_matrix(graph)
```

```{python}
start_time = time.time()
locality = np.zeros(len(neurons_dataframe))
locality[0] = 1        
page=pagerank_power(SM, p=0.85, personalize=locality, tol=1e-6)
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
start_time = time.time()
local_pagerank_results = {}
for i in range(len(neurons_dataframe)):
    locality = np.zeros(len(neurons_dataframe))
    locality[i] = 1
    page = pagerank_power(SM, p=0.85, personalize=locality, tol=1e-6)
    local_pagerank_results[i] = page
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
import pickle
f = open("pagerank_computed.pkl","wb")
pickle.dump(local_pagerank_results,f)
f.close()
```

```{python}
print(page)
print(len(page))
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
        
    plt.hist(page.values(), bins=100)
    plt.show()
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank_scipy(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
    
    plt.hist(page.values(), bins=100)
    plt.show()
```

## Approximate visualization of main brain regions

```{python}
names = ['GNG', 'PENP', 'VMNP', 'CX', 'LX', 'AL', 'MB', 'INP', 'VLNP', 'OL', 'SNP', 'LH']
links = [('GNG', 'PENP'), ('PENP', 'VMNP'), ('PENP', 'CX'), ('PENP', 'LX'), ('VMNP', 'CX'),
         ('VMNP', 'LX'), ('CX', 'LX'), ('CX', 'AL'), ('LX', 'AL'), ('AL', 'INP'), 
         ('AL', 'MB'), ('INP', 'VLNP'), ('VLNP', 'OL'), ('MB', 'SNP'), ('SNP', 'LH')]
```

```{python}
high_lvl_brain = nx.Graph()
high_lvl_brain.add_nodes_from(names)
high_lvl_brain.add_edges_from(links)
high_lvl_brain_ig = ig.Graph.from_networkx(high_lvl_brain)
```

```{python}
colors = ig.drawing.colors.known_colors
colors = list(colors.keys())
print(colors)
```

```{python}
#set label to be names of nx graph nodes
high_lvl_brain_ig.vs["label"] = high_lvl_brain_ig.vs["_nx_name"]

visual_style = {}
#node size
visual_style["vertex_size"] = 20
#node color
c = [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0]
n_names = np.array(['deep sky blue', 'magenta4'])
visual_style["vertex_color"] = n_names[c]  
#node label
visual_style["vertex_label"] = high_lvl_brain_ig.vs["label"]
#node label color
visual_style["vertex_label_color"] = "black"
#node label size
visual_style["vertex_label_size"] = 15
#edge thickness
visual_style["edge_width"] = 2
#bounding box
visual_style["bbox"] = (500, 500)
#margin
visual_style["margin"] = 20

ig.plot(high_lvl_brain_ig, "high_lvl_brain.pdf", **visual_style, layout="kk")
```

# To do list
1. ‚úîÔ∏è list brain regions to create different $12$ theoretical communities;
1. ‚úîÔ∏è understand what the $12$ regions does; 
1. üü• Understand brain section of neurons;
1. ‚úîÔ∏è Check degree distribution is power law
1. ‚óªÔ∏è netorkx documentation (algorithms in particular);
1. ‚óªÔ∏è Local connectivity measure;
1. ‚óªÔ∏è Colour adjacency matrix based on connection supergroup **FRANK**;
1. ‚óªÔ∏è Redefine ROI by taking connection with highest weight only **FRANK**;
1. ‚óªÔ∏è Define coarse graining procedure **BALLA**;
1. ‚óªÔ∏è Curare il report up to now;
1. ‚óªÔ∏è Better define coarse graining metrics;
1. ‚óªÔ∏è Community detection (naive and after coarse graining);
1. ‚óªÔ∏è Betweenness (especially for multiple ROI synapses), centrality, small world characteristics;
1. ‚úîÔ∏èDistance from random network;
1. ‚óªÔ∏è robustness to cuts. In particular it may be interesting to attack outer regions of the brain, the more easily damaged. The  **common connectome constraint paper** already say something about it;


###### Problems
In point (3), we have that the same synapse (same link between two neurons, as we've defined it here) may belong to more than one region. We could add only the one with the highest weight, but often is not possible. Random?


## Average measures


#### Degree

```{python}
degree = np.array([ d for n, d in graph.degree()])
degree_mean = int(degree.mean())
print(f"Degree mean: {degree_mean}")
y, x = np.histogram(degree, bins=100)
x = (x[1:]+x[:-1])/2
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

#### Degree connectivity
The *average degree connectivity* is the average nearest neighbor degree of nodes with degree k. 

```{python}
avg_degree_connectivity = nx.average_degree_connectivity(graph)
avg_degree_connectivity = np.array([i for k,i in avg_degree_connectivity.items()])
print(f"Global average degree connectivity value: {int(avg_degree_connectivity.mean())}")

plt.hist(avg_degree_connectivity, bins=200)
plt.grid()
plt.title("Average degree connectivity distribution")
plt.xlabel("Average degree connectivity for degree k")
plt.show()
```

#### Average clustering coefficient
Time saver: avg_clustering_coeff = 0.31370364316752

```{python}
if PATIENCE:
    start_time = time.time()

    avg_clustering_coeff = nx.average_clustering(graph)
    print(f"Average clustering coefficient: {avg_clustering_coeff}")

    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Average shortest path length (NOTE: veeeery long)

```{python}
if PATIENCE:
    # sanity check
    if nx.is_connected(graph):
        start_time = time.time()

        avg_shortest_path = nx.average_shortest_path_length(graph)
        print(avg_shortest_path)

        end_time = time.time()
        print("Total time: {:.2f} seconds".format((end_time - start_time)))
    else:
        print("Graph is not connected.")
```

```{python}

```
