---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.7.1
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
# Network analysis
import networkx as nx
from scipy import sparse
from fast_pagerank import pagerank_power
# Data handling
import pandas as pd
# Visualization
import matplotlib.pyplot as plt
import igraph as ig
import cairocffi as cairo
# Miscellaneous
import time
import os
import numpy as np
```

```{python}
# False -> avoid running time-consuming cells
PATIENCE = False
```

## Importing datasets
- neurons_dataframe contains neuron number (body_id). The body_id is an [unique identifier](https://en.wikipedia.org/wiki/Unique_identifier#:~:text=A%20unique%20identifier%20(UID)%20is,with%20an%20atomic%20data%20type.). It also contains the neuron cell type (type) and the neuron unique name (instance).
- synapses_dataframe contains the sparse connectivity matrix of the network;
- roi_dataframe contains also the region of interest of the connection, but some of them are repeated, i.e. we have the same couple of connecting neurons in different regions (maybe they are not sure)

https://asajadi.github.io/fast-pagerank/

```{python}
neurons_dataframe = pd.read_csv('exported-traced-adjacencies/traced-neurons.csv')
synapses_dataframe = pd.read_csv('exported-traced-adjacencies/traced-total-connections.csv')
roi_dataframe = pd.read_csv('exported-traced-adjacencies/traced-roi-connections.csv')
```

```{python}
neurons_dataframe.head()
```

```{python}
# add link as 2-tuple between neurons bodyId
synapses_dataframe['synaps'] = list(zip(synapses_dataframe.bodyId_pre, synapses_dataframe.bodyId_post))
print(synapses_dataframe.head())
print(f"Number of unique synapses: {synapses_dataframe['synaps'].nunique()}")
print(f"Length of synapses dataset: {len(synapses_dataframe)}")
# NOTE: the weight of the synapses dataset is the sum of all the weights from the ROI
```

#### Hemibrain regions of interest

```{python}
roi_dataframe['synaps'] = list(zip(roi_dataframe.bodyId_pre, roi_dataframe.bodyId_post))
cols = list(roi_dataframe.columns)  # to use later
print(roi_dataframe.head())
print(f"Number of unique synapses: {roi_dataframe['synaps'].nunique()}")
print(f"Length of ROI dataset: {len(roi_dataframe)}")
print(f"Difference: {len(roi_dataframe) - roi_dataframe['synaps'].nunique()}")
```

```{python}
roi = roi_dataframe.to_numpy()
indexes_to_delete = []
out_index = 0
hhh = 0

start_time = time.time()

while out_index < len(roi_dataframe):
    j = 1
    while(out_index+j < len(roi_dataframe) and roi[out_index][4] == roi[out_index + j][4]):
        indexes_to_delete.append(out_index+j)
        j = j + 1
    out_index = out_index + j

roi = np.delete(roi, obj=indexes_to_delete, axis=0)

end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
roi_dataframe = pd.DataFrame(roi, columns=list(roi_dataframe.columns))
print(roi_dataframe.head())

# sanity check
assert roi_dataframe['synaps'].nunique() == len(roi_dataframe)
```

1. ◻️ Colour adjacency matrix based on connection supergroup **FRANK**;
1. ◻️ Redefine ROI by taking connection with highest weight only **FRANK**;


```{python}
# key is superset, value is list of roi
roi_supersets = {
    'AL': ['AL(L)', 'AL(R)'],
    'CX': ['FB', 'EB', 'PB', 'NO', 'AB(L)', 'AB(R)'],
    'GNG': ['GNG'],
    'INP': ['CRE(L)', 'CRE(R)', 'SCL(L)', 'SCL(R)', 'IB', 'ICL(L)', 'ICL(R)', 'ATL(L)', 'ATL(R)', 'IB'],
    'LH': ['LH(R)'],
    'LX': ['BU(L)', 'BU(R)','LAL(L)', 'LAL(R)'],
    'MB': ['CA(R)','PED(R)',"a'L(L)", "a'L(R)", 'aL(L)', 'aL(R)', "b'L(L)", "b'L(R)", 'bL(L)', 'bL(R)', 'gL(L)', 'gL(R)'],
    'OL': ['AME(R)','ME(R)','LO(R)', 'LOP(R)'],
    'PENP': ['SAD','CAN(R)','FLA(R)'],
    'SNP': ['SIP(L)', 'SIP(R)', 'SLP(R)', 'SMP(L)', 'SMP(R)', 'SPS(L)', 'SPS(R)'],
    'VLNP': ['AOTU(R)','AVLP(R)','PVLP(R)','PLP(R)','WED(R)'],
    'VMNP': ['VES(L)', 'VES(R)','EPA(L)', 'EPA(R)','GOR(L)', 'GOR(R)','SPS(L)', 'SPS(R)','IPS(R)'],
}

def map_roi_to_superset(links):
    def find_attr(roi):
        for key, value in roi_supersets.items():
            if roi in value:
                return key
    res = list(map(find_attr, links))
    return res

roi_dataframe['superset'] = map_roi_to_superset(roi_dataframe['roi'].tolist())
roi_dataframe.head()
```

```{python}
# sanity check
roi = list(roi_dataframe['roi'].unique())
removed = []
for elem in roi:
    for key, value in roi_supersets.items():
        if elem in value and elem not in removed:
            removed.append(elem)
remains = [x for x in roi if x not in removed]
print(f"Non-included sections: {remains}")
```

```{python}
roi_dataframe.to_csv(os.path.join("exported-traced-adjacencies", "treated_roi.csv"))
```

## Graph

```{python}
# neuron type is attribute of the neuron in the graph, we directly extract it from the dataframe
nodes = neurons_dataframe[['bodyId', 'type']]
nodes_attr_dict = nodes.set_index('bodyId')['type'].to_dict()

links = roi_dataframe[['synaps', 'roi', 'superset']]
links_attr_dict_1 = links.set_index('synaps')['roi'].to_dict()
links_attr_dict_2 = links.set_index('synaps')['superset'].to_dict()
```

```{python}
graph = nx.Graph()
graph.add_nodes_from(neurons_dataframe['bodyId'])
nx.set_node_attributes(graph, nodes_attr_dict, 'type')
graph.add_edges_from(roi_dataframe['synaps'])
nx.set_edge_attributes(graph, links_attr_dict_1, 'roi')
nx.set_edge_attributes(graph, links_attr_dict_2, 'superset')
```

```{python}
# sanity check
print(f"Node: {list(graph.nodes)[0]}")
print(f"Node attributes: {graph.nodes[200326126]}")

print(f"Edge: {list(graph.edges)[0]}")
print(f"Edge attributes: {graph.edges[(200326126, 264083994)]}")
```

#### Adjacency matrix

```{python}
dense=nx.adjacency_matrix(graph).todense()
```

```{python}
matfig = plt.figure(figsize=(20,20))
plt.matshow(nx.adjacency_matrix(graph).todense(), fignum=matfig.number, cmap=plt.get_cmap("binary"))
```

#### Non-randomness coefficient

```{python}
if PATIENCE:
    start_time = time.time()
    loc, glob = nx.non_randomness(graph)
    print(glob)
    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Degree distribution and properties


See https://stackoverflow.com/questions/49908014/how-can-i-check-if-a-network-is-scale-free and https://pypi.org/project/powerlaw/

I'm not completely sure about the parameters.

```{python}
degree = np.array([ d for n, d in graph.degree()])
print(degree.mean())
```

```{python}
# used for degree distribution and powerlaw test
# reverse = True to have the cumulative distribution
degree_sequence = sorted([d for n, d in graph.degree()], reverse=True) 
# Power laws are probability distributions with the form:p(x)∝x−α
import powerlaw 
# NOTE: xmin is the data value beyond which distributions should be fitted. 
# If None an optimal one will be calculated
fit = powerlaw.Fit(degree_sequence) 
fig2 = fit.plot_pdf(color='b', linewidth=2)
fit.power_law.plot_pdf(color='g', linestyle='--', ax=fig2)
plt.legend(["data", "power law"])
R, p = fit.distribution_compare('power_law', 'exponential', normalized_ratio=True)
print (f"Loglikelihood: {R:.2f}, p-value: {p:.2e}")
```

```{python}
plt.figure(figsize=(10, 6))
fit.distribution_compare('power_law', 'lognormal')
fig4 = fit.plot_ccdf(linewidth=3, color='black')
fit.power_law.plot_ccdf(ax=fig4, color='r', linestyle='--') #powerlaw
fit.lognormal.plot_ccdf(ax=fig4, color='g', linestyle='--') #lognormal
fit.stretched_exponential.plot_ccdf(ax=fig4, color='b', linestyle='--') #stretched_exponential
plt.legend(["data", "power law", "log-normal", "stretched_exponential"])
```

```{python}
#bins = np.logspace(0, 4, 100)
bins = 100
y, x = np.histogram(degree, bins=bins)
x = (x[1:]+x[:-1])/2
```

```{python}
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

# Coarse graining 
To apply our coarse graining procedure we aggregate the nodes with their nearest neighbors, where the concept of nearest is choosen using one of the following centrality measures:
- **local page rank**. The local page rank gives the proximity of the nodes with respect to the chosen node. So the second highest node is gonna be merged with the one where the lpr is computed. Networkx creates each time the sparse matrix, which is a time consuming task. We need to compute the page rank several times, and so chosen to use another representation;
- **link strength**. We merge the more connected or the less connected (with the idea that they are not influencing the graph properties)


#### PageRank measure

```{python}
SM = nx.convert_matrix.to_scipy_sparse_matrix(graph)
```

```{python}
start_time = time.time()
locality = np.zeros(len(neurons_dataframe))
locality[0] = 1        
page=pagerank_power(SM, p=0.85, personalize=locality, tol=1e-6)
end_time = time.time()
print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
        
    plt.hist(page.values(), bins=100)
    plt.show()
```

```{python}
# compute pagerank ranking
if PATIENCE:
    start_time = time.time()

    page = nx.algorithms.link_analysis.pagerank_scipy(graph)

    end_time = time.time()
    print("Total time: {:.2f} minutes".format((end_time - start_time)/60))
    
    plt.hist(page.values(), bins=100)
    plt.show()
```

## Approximate visualization of main brain regions

```{python}
names = ['GNG', 'PENP', 'VMNP', 'CX', 'LX', 'AL', 'MB', 'INP', 'VLNP', 'OL', 'SNP', 'LH']
links = [('GNG', 'PENP'), ('PENP', 'VMNP'), ('PENP', 'CX'), ('PENP', 'LX'), ('VMNP', 'CX'),
         ('VMNP', 'LX'), ('CX', 'LX'), ('CX', 'AL'), ('LX', 'AL'), ('AL', 'INP'), 
         ('AL', 'MB'), ('INP', 'VLNP'), ('VLNP', 'OL'), ('MB', 'SNP'), ('SNP', 'LH')]
```

```{python}
high_lvl_brain = nx.Graph()
high_lvl_brain.add_nodes_from(names)
high_lvl_brain.add_edges_from(links)
high_lvl_brain_ig = ig.Graph.from_networkx(high_lvl_brain)
```

```{python}
colors = ig.drawing.colors.known_colors
colors = list(colors.keys())
#print(colors)
```

```{python}
#set label to be names of nx graph nodes
high_lvl_brain_ig.vs["label"] = high_lvl_brain_ig.vs["_nx_name"]

visual_style = {}
#node size
visual_style["vertex_size"] = 20
#node color
c = [0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0]
n_names = np.array(['deep sky blue', 'magenta4'])
visual_style["vertex_color"] = n_names[c]  
#node label
visual_style["vertex_label"] = high_lvl_brain_ig.vs["label"]
#node label color
visual_style["vertex_label_color"] = "black"
#node label size
visual_style["vertex_label_size"] = 15
#edge thickness
visual_style["edge_width"] = 2
#bounding box
visual_style["bbox"] = (500, 500)
#margin
visual_style["margin"] = 20

ig.plot(high_lvl_brain_ig, "high_lvl_brain.pdf", **visual_style, layout="kk")
```

# To do list
1. ✔️ list brain regions to create different $12$ theoretical communities;
1. ✔️ understand what the $12$ regions does; 
1. 🟥 Understand brain section of neurons;
1. ✔️ Check degree distribution is power law
1. ◻️ netorkx documentation (algorithms in particular);
1. ◻️ Local connectivity measure;
1. ◻️ Colour adjacency matrix based on connection supergroup **FRANK**;
1. ✔️ Redefine ROI by taking connection with highest weight only **FRANK**;
1. ◻️ Define coarse graining procedure **BALLA**;
1. ◻️ Curare il report up to now;
1. ◻️ Better define coarse graining metrics;
1. ◻️ Community detection (naive and after coarse graining);
1. ◻️ Betweenness (especially for multiple ROI synapses), centrality, small world characteristics;
1. ✔️Distance from random network;
1. ◻️ robustness to cuts. In particular it may be interesting to attack outer regions of the brain, the more easily damaged. The  **common connectome constraint paper** already say something about it;


###### Problems
~In point (3), we have that the same synapse (same link between two neurons, as we've defined it here) may belong to more than one region. We could add only the one with the highest weight, but often is not possible. Random?~ Fixed by taking the highest-weight only connection.


## Average measures


#### Degree

```{python}
degree = np.array([ d for n, d in graph.degree()])
degree_mean = int(degree.mean())
print(f"Degree mean: {degree_mean}")
y, x = np.histogram(degree, bins=100)
x = (x[1:]+x[:-1])/2
# plot degree bins
plt.loglog()
plt.plot(x, y, 'ro')
plt.xlabel('Log Degree log(k)')
plt.ylabel('Log Frequency ')
plt.title('Degree distribution')
plt.grid()
plt.show()
```

#### Degree connectivity
The *average degree connectivity* is the average nearest neighbor degree of nodes with degree k. 

```{python}
avg_degree_connectivity = nx.average_degree_connectivity(graph)
avg_degree_connectivity = np.array([i for k,i in avg_degree_connectivity.items()])
print(f"Global average degree connectivity value: {int(avg_degree_connectivity.mean())}")

plt.hist(avg_degree_connectivity, bins=200)
plt.grid()
plt.title("Average degree connectivity distribution")
plt.xlabel("Average degree connectivity for degree k")
plt.show()
```

#### Average clustering coefficient
Time saver: avg_clustering_coeff = 0.31370364316752

```{python}
if PATIENCE:
    start_time = time.time()

    avg_clustering_coeff = nx.average_clustering(graph)
    print(f"Average clustering coefficient: {avg_clustering_coeff}")

    end_time = time.time()
    print("Total time: {:.2f} seconds".format((end_time - start_time)))
```

#### Average shortest path length (NOTE: veeeery long)

```{python}
if PATIENCE:
    # sanity check
    if nx.is_connected(graph):
        start_time = time.time()

        avg_shortest_path = nx.average_shortest_path_length(graph)
        print(avg_shortest_path)

        end_time = time.time()
        print("Total time: {:.2f} seconds".format((end_time - start_time)))
    else:
        print("Graph is not connected.")
```

```{python}

```
